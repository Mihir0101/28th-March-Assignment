{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab93f9c9-5467-496a-9616-f905242f3c7c",
   "metadata": {},
   "source": [
    "# 28th March Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b49966-b9d4-4e55-ae4f-a09d85fea864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9978ee6-df3f-4c5a-9e0f-a99c74b0b68f",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c007146-8cbd-464c-8d0f-c964a85fe3d2",
   "metadata": {},
   "source": [
    "- > Ridge regression is also a part of regression algorithms.\n",
    "\n",
    "- > The main use of Ridge regression is to reduce overfitting.\n",
    "\n",
    "* Differences :\n",
    "\n",
    "- > It adds a panalty term in the simple cost function.\n",
    "\n",
    "- > New parameters in cost function of ridge regression is Lambda and Theta1 value.\n",
    "\n",
    "- > Cost function decrease the theta value but not make it zero.\n",
    "\n",
    "- > Because of panalty parameter training dataset give some error,but that is good for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926910b-a4ad-415a-8f51-8b44e179a73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31157627-29d8-4a13-b7b6-6d67b6f4e2e4",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3c790-4c55-49b4-aba7-644ff24222e1",
   "metadata": {},
   "source": [
    "- > There is a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "- > The errors have constant variance (homoscedasticity).\n",
    "\n",
    "- > The errors are independent of each other.\n",
    "\n",
    "- > The errors are normally distributed .\n",
    "\n",
    "- > There is no multicollinearity among the independent variables.\n",
    "\n",
    "- > The number of predictors is less than the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb918cd-c39d-4ab1-9dc4-26821a45ced5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bade887f-62bf-47a9-820b-b06674609140",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbc20f-4b4f-455a-b8c4-a02c375c533e",
   "metadata": {},
   "source": [
    "* There are different techniques for select the value of hyper parameter tuning.\n",
    "\n",
    "1. Cross Validation \n",
    "\n",
    "- > Common cross validation method is k fold cross validation.In this method we have to divide our dataset into k folds.After dividing the dataset we have to train model on k1 fold and validate it on remaining folds.We have to do this process k time through changing the validation set.\n",
    "\n",
    "2. Evaluate Performance \n",
    "\n",
    "- > After fitting the Ridge Regression models for different λ values, evaluate their performance on the validation set using an appropriate metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d25b2-5a46-45cb-95b0-4d7828cb48ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1819c059-4e85-4de8-93ee-a0302940bf7f",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24bd9d1-8283-4901-b418-50a7b8a02c80",
   "metadata": {},
   "source": [
    "- > Ridge rigression is used to solve the problem of overfitting.\n",
    "\n",
    "- > It reduces the slope value through its unique cost function.\n",
    "\n",
    "- > We can use Lasso regression for feature selection instead of ridge regression.\n",
    "\n",
    "- > Lasso regression sets coefficients exactly to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641099cd-c8fd-4585-b270-3597405a71ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f686a3-9896-4aeb-beac-baab76b6bd95",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f7942-af7d-4e4a-818b-6757e639bd9d",
   "metadata": {},
   "source": [
    "- > Ridge Regression is specifically designed to handle multicollinearity.\n",
    "\n",
    "- > It penalize the value of coefficients by regularization term.\n",
    "\n",
    "- > Ridge regression reduces the impact of multicolinearity by distributing shrinkage on all variables.\n",
    "\n",
    "- > Shrinkage of coefficient depends uppon hyperparameter tuning,which is also know as lambda.\n",
    "\n",
    "- > The more high value of lambda the more it will shrink the coefficients.\n",
    "\n",
    "- > We should know that the ridge regression doesn't solve the problem of multicollinearity properly when correlation between variables is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776779db-a8ea-4b0f-85ec-7d49ace34112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87fd7ed5-2541-4803-bec7-78546422842f",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30d89e-fc01-4872-873a-90c60d5e8626",
   "metadata": {},
   "source": [
    "- > Yes, Ridge Regression can handle both categorical and continuous independent variables, but some considerations need to be taken into account.\n",
    "\n",
    "* Continuous Variables:\n",
    "\n",
    "- > Ridge Regression is well-suited for handling continuous independent variables, as it is originally designed for linear regression problems with numerical features.\n",
    "\n",
    "- > The regularization term in Ridge Regression penalizes the magnitude of coefficients associated with continuous variables, helping to prevent overfitting and stabilize the model.\n",
    "\n",
    "* Categorical Variables:\n",
    "\n",
    "- > Ridge Regression can be extended to handle categorical variables by encoding them appropriately.\n",
    "\n",
    "- > One common method is to use one-hot encoding for categorical variables. This involves creating binary (0/1) dummy variables for each category of the categorical variable. Each dummy variable indicates the presence or absence of a specific category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ecb6e-e8da-44c7-a75e-4f9b36f5b6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3789f36-be7f-4ca7-8453-203ad9557559",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985ff94-673d-433c-9705-f635c6421e5e",
   "metadata": {},
   "source": [
    "- > Interpreting the coefficients of Ridge Regression involves understanding the effect of each independent variable on the dependent variable while considering the regularization term. Ridge Regression introduces a penalty term that shrinks the coefficients toward zero, impacting their magnitudes. \n",
    "\n",
    "- > The coefficients in Ridge Regression are penalized to be smaller than they would be in a standard linear regression model (OLS). The larger the regularization parameter (λ), the stronger the shrinkage, and the closer the coefficients will be to zero.\n",
    "\n",
    "- > The sign of the coefficients remains interpretable in Ridge Regression. A positive coefficient indicates a positive relationship with the dependent variable, while a negative coefficient indicates a negative relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1616d4-1914-430e-b083-f7cf544f181b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f25b27-6fb3-4349-ac4c-23f98bc744f9",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fffc90-f53f-4937-85e0-cbe1dddfabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "- > Yes, Ridge Regression can be applied to time-series data analysis, but there are some considerations and challenges specific to time-series modeling. Time-series data typically involves observations taken at successive points in time, and the temporal ordering of data points must be maintained. Here's how Ridge Regression can be used for time-series data:\n",
    "\n",
    "- > Ensure that the temporal order of the data is maintained. In time-series analysis, the order of observations matters, and you should avoid shuffling the data.\n",
    "Feature Engineering:\n",
    "\n",
    "Construct relevant features for time-series analysis. These might include lagged values of the target variable or other features. Lagged values capture the temporal dependencies in the data.\n",
    "Consider creating additional features such as moving averages, differences (for stationarity), or other transformations that capture patterns in the time series.\n",
    "Handling Autocorrelation:\n",
    "\n",
    "Time-series data often exhibits autocorrelation, where values at one time point are correlated with values at nearby time points. Ridge Regression can help mitigate multicollinearity due to autocorrelation, but it may not fully capture complex temporal dependencies.\n",
    "Techniques like autoregressive models (ARIMA), seasonal decomposition, or incorporating lagged terms explicitly into the model can be beneficial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
